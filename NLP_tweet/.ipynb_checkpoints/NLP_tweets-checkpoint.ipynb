{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of the project is to build a model to detect toxic or non-toxic comments.\n",
    "\n",
    "Metrics *F1* should be at least 0.75. \n",
    "\n",
    "Dataset:\n",
    "Column *text* has comments and *toxic* â€” target feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import re \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "import spacy\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer \n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "tokenizer=TweetTokenizer()\n",
    "eng_stopwords = set(nltk_stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload data as df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      "text     159571 non-null object\n",
      "toxic    159571 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the target feature balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of non-toxic comments: 89.83%\n",
      "Amount of toxic comments: 10.17%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['toxic'].hist(bins = 30)\n",
    "plt.suptitle('Amount of toxic and non-toxic comments')\n",
    "print('Amount of non-toxic comments: {:.2%}'.format(df.toxic[df['toxic']==0].count()/df.shape[0]))\n",
    "print('Amount of toxic comments: {:.2%}'.format(1 - df.toxic[df['toxic']==0].count()/df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have quite imbalance target feature. We have to take into consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will conduct an experiment and train the models on raw data and on stemmed data. Let's look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df.toxic\n",
    "text = df.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created our target and comment text. Let's now try to process our corpus. The following ideas for cleaning up comments and parts of the code are taken from the kaggle platform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://drive.google.com/file/d/0B1yuv8YaUVlZZ1RzMFJmc1ZsQmM/view\n",
    "# library to change the abbreviations\n",
    "APPO = {\n",
    "\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\",\n",
    "\"tryin'\":\"trying\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(comment):\n",
    "    \"\"\"\n",
    "    This function goes line by line abd clean the text\n",
    "    \"\"\"\n",
    "    #low register\n",
    "    comment=comment.lower()\n",
    "    #delete \\n\n",
    "    comment=re.sub(\"\\\\n\",\" \",comment)\n",
    "    #delete IP \n",
    "    comment=re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",\"\",comment)\n",
    "    #delete users name\n",
    "    comment=re.sub(\"\\[\\[.*\\]\",\"\",comment)\n",
    "    comment = re.sub(r\"\\d\", \"\", comment)\n",
    "    #break comments on words\n",
    "    words=tokenizer.tokenize(comment)\n",
    "    \n",
    "    # exchage the abbreviation with library APPO \n",
    "    words=[APPO[word] if word in APPO else word for word in words]\n",
    "    words=[lem.lemmatize(word, \"v\") for word in words]\n",
    "    words = [w for w in words if not w in eng_stopwords]\n",
    "    \n",
    "    clean_sent=\" \".join(words)\n",
    "    # delete double space and symbols\n",
    "    clean_sent=re.sub(\"\\W+\",\" \",clean_sent)\n",
    "    clean_sent=re.sub(\"  \",\" \",clean_sent)\n",
    "    return(clean_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'explanation edit make username hardcore metallica fan revert were not vandalisms closure gas vote new york dolls fac please do not remove template talk page since I am retire '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean(df.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text=df['text'].apply(lambda x :clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Explanation\\nWhy the edits made under my usern...\n",
       "1    D'aww! He matches this background colour I'm s...\n",
       "2    Hey man, I'm really not trying to edit war. It...\n",
       "3    \"\\nMore\\nI can't make any real suggestions on ...\n",
       "4    You, sir, are my hero. Any chance you remember...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    explanation edit make username hardcore metall...\n",
       "1    d aww match background colour I am seemingly s...\n",
       "2    hey man I am really try edit war it is guy con...\n",
       "3     cannot make real suggestions improvement wond...\n",
       "4               sir hero chance remember page that is \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have prepared the data for checking different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use TF-IDF method. Let us check raw data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, target_train, target_test = train_test_split(text, target, test_size=0.3,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 111699\n",
      "Test dataset size: 47872\n"
     ]
    }
   ],
   "source": [
    "print('Train dataset size:', features_train.shape[0])\n",
    "print('Test dataset size:', features_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf = TfidfVectorizer(strip_accents='unicode',stop_words=eng_stopwords)\n",
    "tf_idf = count_tf_idf.fit_transform(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111699, 151729)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_test = count_tf_idf.transform(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created the features, let us use logistic regression and check F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scorer = make_scorer(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=LogisticRegression(C=1.0, class_weight='balanced',\n",
       "                                          dual=False, fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='warn',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=None, solver='warn',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'C': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       "                         'max_iter': array([ 50,  75, 100, 125])},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring=make_scorer(f1_score), verbose=0)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'C'                :np.arange(0.1, 1.1, 0.1),\n",
    "    'max_iter'         :np.arange(50, 150, 25)\n",
    " \n",
    "}\n",
    " \n",
    "logit_model = GridSearchCV(LogisticRegression(class_weight = \"balanced\"), param_grid = parameters, cv = 3, scoring = f1_scorer, return_train_score=True)\n",
    "logit_model.fit(tf_idf, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=50, multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(class_weight = \"balanced\",max_iter=50,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score train: 0.8451361272560416\n"
     ]
    }
   ],
   "source": [
    "lr.fit(tf_idf, target_train)\n",
    " \n",
    "y_pred = lr.predict(tf_idf)\n",
    "print('F1-score train:', f1_score(target_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score  test: 0.7502539008401808\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = lr.predict(tf_idf_test)\n",
    "print('F1-score  test:', f1_score(target_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have reached the aim metrics. Let us check on clean data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_clean_train, features_clean_test, target_train, target_test = train_test_split(clean_text, target, test_size=0.3,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_clean = count_tf_idf.fit_transform(features_clean_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score train: 0.8355218216318786\n"
     ]
    }
   ],
   "source": [
    "lr.fit(tf_idf_clean, target_train)\n",
    " \n",
    "y_pred_clean = lr.predict(tf_idf_clean)\n",
    "print('F1-score train:', f1_score(target_train, y_pred_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_test_clean = count_tf_idf.transform(features_clean_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score  test: 0.7479406919275123\n"
     ]
    }
   ],
   "source": [
    "y_pred_test_clean = lr.predict(tf_idf_test_clean)\n",
    "print('F1-score  test:', f1_score(target_test, y_pred_test_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see better results with the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check RandomForest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    " model_trees = RandomForestClassifier(class_weight = 'balanced',random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       n_estimators=10, n_jobs=None, oob_score=False,\n",
       "                       random_state=1234, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trees.fit(tf_idf, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score  test: 0.9686026142029377\n"
     ]
    }
   ],
   "source": [
    "print('F1-score  test:',f1_score(target_train, model_trees.predict(tf_idf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score  test: 0.5891778303125821\n"
     ]
    }
   ],
   "source": [
    "print('F1-score  test:', f1_score(target_test, model_trees.predict(tf_idf_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our model is overfitting. Let us check clean data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       n_estimators=10, n_jobs=None, oob_score=False,\n",
       "                       random_state=1234, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trees.fit(tf_idf_clean, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score  test: 0.9682125168842864\n"
     ]
    }
   ],
   "source": [
    "print('F1-score  test:',f1_score(target_train, model_trees.predict(tf_idf_clean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score  test: 0.5952680946381071\n"
     ]
    }
   ],
   "source": [
    "print('F1-score  test:', f1_score(target_test, model_trees.predict(tf_idf_test_clean)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1-score has improved, but still far from logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have analyzed our data and built a model based on logistic regression with an F1 metric of at least 0.75. It turned out that the raw data worked better based on TF-IDF and logistic regression. The cleaned data performed worse. \n",
    "Random forest did not improve our performance, the model is retrained and produces worse results on the test sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
